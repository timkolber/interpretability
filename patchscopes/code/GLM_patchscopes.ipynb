{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJYfysaREkb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mdEmY4rDQ3ik",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from general_utils import (\n",
    "  ModelAndTokenizer,\n",
    ")\n",
    "from patchscopes_utils import set_hs_patch_hooks_t5, set_hs_patch_hooks_neox, set_hs_patch_hooks_gptj, set_hs_patch_hooks_llama, evaluate_patch_next_token_prediction\n",
    "from tqdm import tqdm\n",
    "torch.set_grad_enabled(False)\n",
    "sns.set_theme(context=\"notebook\",\n",
    "        rc={\"font.size\":16,\n",
    "            \"axes.titlesize\":16,\n",
    "            \"axes.labelsize\":16,\n",
    "            \"xtick.labelsize\": 16.0,\n",
    "            \"ytick.labelsize\": 16.0,\n",
    "            \"legend.fontsize\": 16.0})\n",
    "palette_ = sns.color_palette(\"Set1\")\n",
    "palette = palette_[2:5] + palette_[7:]\n",
    "sns.set_theme(style='whitegrid')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LQX9Qx6GGdLZ"
   },
   "outputs": [],
   "source": [
    "model_to_hook = {\n",
    "    \"EleutherAI/pythia-12b\": set_hs_patch_hooks_neox,\n",
    "    \"meta-llama/Llama-2-13b-hf\": set_hs_patch_hooks_llama,\n",
    "    \"lmsys/vicuna-7b-v1.5\": set_hs_patch_hooks_llama,\n",
    "    \"./stable-vicuna-13b\": set_hs_patch_hooks_llama,\n",
    "    \"CarperAI/stable-vicuna-13b-delta\": set_hs_patch_hooks_llama,\n",
    "    \"EleutherAI/gpt-j-6b\": set_hs_patch_hooks_gptj,\n",
    "    \"plenz/GLM-t5-3b\": set_hs_patch_hooks_t5,\n",
    "    \"google-t5/t5-3b\": set_hs_patch_hooks_t5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hugging Face cache directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/students/kolber/seminars/kolber/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4479f16b9a544b79bb8790693701d8de",
      "708c70001ba64b3196bf2ced4fe01f6c"
     ]
    },
    "id": "fKGGJO3GQ3in",
    "outputId": "aed82adb-d542-4de6-ade7-c2a4f7aadcc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/kolber/miniconda3/envs/GLM/lib/python3.9/site-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5EncoderModel(\n",
       "    (shared): Embedding(32128, 1024)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 1024)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (relative_attention_bias): Embedding(35, 32)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "model_name = \"facebook/t5-3b\"\n",
    "sos_tok = False\n",
    "\n",
    "if \"13b\" in model_name or \"12b\" in model_name:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None\n",
    "\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "mt.set_hs_patch_hooks = model_to_hook[model_name]\n",
    "mt.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsZLB6l_GdLa"
   },
   "source": [
    "# Next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(input_ids=tensor([[   19,     3,     9,    19,     3,     9,    19,     3,     9,  2586,\n",
      "          1001,     3,   102, 14957,  1712,  1782, 21603,    10,    71,  5963,\n",
      "            19,    46,  2586,     5,     1]]), relative_position=tensor([[[ 0,  1,  2,  0,  0,  0,  0,  0,  0,  0, -4, -3, -2, -1,  0,  3,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [-1,  0,  1,  0,  0,  0,  0,  0,  0,  0, -5, -4, -3, -2,  0,  2,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [-2, -1,  0,  0,  0,  0,  0,  0,  0,  0, -6, -5, -4, -3,  0,  1,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 0,  0,  0,  0,  1,  2,  0,  0,  0,  3,  0,  0,  0,  0,  0, -1,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 0,  0,  0, -1,  0,  1,  0,  0,  0,  2,  0,  0,  0,  0,  0, -2,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 0,  0,  0, -2, -1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0, -3,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  1,  2,  3,  0,  0,  0,  0, -1,  0,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 0,  0,  0,  0,  0,  0, -1,  0,  1,  2,  0,  0,  0,  0, -2,  0,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 0,  0,  0,  0,  0,  0, -2, -1,  0,  1,  0,  0,  0,  0, -3,  0,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 0,  0,  0, -3, -2, -1, -3, -2, -1,  0,  0,  0,  0,  0, -4, -4,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 4,  5,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  3,  0,  7,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 3,  4,  5,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1,  2,  0,  6,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 2,  3,  4,  0,  0,  0,  0,  0,  0,  0, -2, -1,  0,  1,  0,  5,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 1,  2,  3,  0,  0,  0,  0,  0,  0,  0, -3, -2, -1,  0,  0,  4,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 0,  0,  0,  0,  0,  0,  1,  2,  3,  4,  0,  0,  0,  0,  0,  0,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [-3, -2, -1,  1,  2,  3,  0,  0,  0,  4, -7, -6, -5, -4,  0,  0,  2,\n",
      "           2,  2,  2,  2,  2,  2,  2,  2],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "           1,  2,  3,  4,  5,  6,  7,  8],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
      "           0,  1,  2,  3,  4,  5,  6,  7],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -2,\n",
      "          -1,  0,  1,  2,  3,  4,  5,  6],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -3,\n",
      "          -2, -1,  0,  1,  2,  3,  4,  5],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -4,\n",
      "          -3, -2, -1,  0,  1,  2,  3,  4],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -5,\n",
      "          -4, -3, -2, -1,  0,  1,  2,  3],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -6,\n",
      "          -5, -4, -3, -2, -1,  0,  1,  2],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -7,\n",
      "          -6, -5, -4, -3, -2, -1,  0,  1],\n",
      "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -8,\n",
      "          -7, -6, -5, -4, -3, -2, -1,  0]]]), sparsity_mask=tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True]]]), use_additional_bucket=tensor([[[False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False, False,  True,  True,  True, False,\n",
      "           True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False, False,  True,  True,  True, False,\n",
      "           True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False, False,  True,  True,  True, False,\n",
      "           True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "           True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "           True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "           True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False,\n",
      "           True,  True,  True,  True, False, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "           True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True,  True, False,\n",
      "          False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False, False, False, False, False]]]), indices={'is a': [(0, 3), (3, 6), (6, 9)], 'animal': [(9, 10)], 'black poodle': [(10, 14)], 'cat': [(14, 15)], 'dog': [(15, 16)]}, is_concept=tensor([[False, False, False, False, False, False, False, False, False,  True,\n",
      "          True,  True,  True,  True,  True,  True]]), concept_indices={'animal': [(9, 10)], 'black poodle': [(10, 14)], 'cat': [(14, 15)], 'dog': [(15, 16)]}, num_additional_buckets=3, is_graph=tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False, False, False]]))\n",
      "generate conditional on encoded graph and text\n",
      "generation 1: A black poodle is a dog.\n",
      "generation 2: <pad><extra_id_0> a animal.<extra_id_1> has<extra_id_2> a\n"
     ]
    }
   ],
   "source": [
    "graph_1 = [\n",
    "    ('black poodle', 'is a', 'dog'),\n",
    "    ('dog', 'is a', 'animal'),\n",
    "    ('cat', 'is a', 'animal')\n",
    "]\n",
    "text_1 = 'summarize: A bird is an animal.'  # with T5 prefix\n",
    "graph_2 = [\n",
    "    ('dog', 'is a', 'animal'),\n",
    "    ('dog', 'has', 'tail'),\n",
    "    ('dog', 'has', 'fur'),\n",
    "    ('fish', 'is a', 'animal'),\n",
    "    ('fish', 'has', 'scales')\n",
    "]\n",
    "text_2 = None  # T5 MLM\n",
    "\n",
    "how = 'global'  # can be 'global' or 'local', depending on whether the local or global GLM should be used. See paper for more details. \n",
    "data_1 = mt.model.encoder.data_processor.encode_graph(tokenizer=mt.tokenizer, g=graph_1, text=text_1, how=how)\n",
    "data_2 = mt.model.encoder.data_processor.encode_graph(tokenizer=mt.tokenizer, g=graph_2, text=text_2, how=how)\n",
    "datas = [data_1, data_2]\n",
    "model_inputs = mt.model.encoder.data_processor.to_batch(data_instances=datas, tokenizer=mt.tokenizer, max_seq_len=None, device='cpu')\n",
    "\n",
    "print(data_1)\n",
    "\n",
    "\n",
    "outputs = mt.model.encoder(**model_inputs)\n",
    "\n",
    "print('generate conditional on encoded graph and text')\n",
    "outputs = mt.model.generate(encoder_outputs=outputs, max_new_tokens=10)\n",
    "\n",
    "print('generation 1:', mt.tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print('generation 2:', mt.tokenizer.decode(outputs[1], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yuKvnceYGdLe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 -> 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_patch_t5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     target_layer \u001b[38;5;241m=\u001b[39m source_layer \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     predicted_token \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_patch_t5\u001b[49m(\n\u001b[1;32m     12\u001b[0m         mt, prompt_source, prompt_target, source_layer, target_layer,\n\u001b[1;32m     13\u001b[0m         position_source, position_target, position_prediction\u001b[38;5;241m=\u001b[39mposition_target\n\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     16\u001b[0m     records\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_layer\u001b[39m\u001b[38;5;124m'\u001b[39m: source_layer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_layer\u001b[39m\u001b[38;5;124m'\u001b[39m: target_layer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: mt\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(predicted_token)})\n\u001b[1;32m     18\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(records)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_patch_t5' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the ID prompt on the validation set of WikiText (with/without mappings)\n",
    "device = mt.model.device\n",
    "prompt_target = \"repeat: cat -> cat\\n1135 -> 1135\\nhello -> hello\\n?\"\n",
    "position_target = -1\n",
    "records = []\n",
    "for source_layer in tqdm(range(int(mt.num_layers))): \n",
    "    prompt_source = \"United States of America.\"\n",
    "    position_source = 3\n",
    "    target_layer = source_layer % 24\n",
    "    print(f\"Layer {source_layer} -> {target_layer}\")\n",
    "    predicted_token = evaluate_patch_next_token_prediction(\n",
    "        mt, prompt_source, prompt_target, source_layer, target_layer,\n",
    "        position_source, position_target, position_prediction=position_target\n",
    "        )\n",
    "\n",
    "    records.append({'source_layer': source_layer, 'target_layer': target_layer, 'token': mt.tokenizer.decode(predicted_token)})\n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
