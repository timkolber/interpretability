{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJYfysaREkb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mdEmY4rDQ3ik",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Scienfitic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Visuals\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\",\n",
    "        rc={\"font.size\":16,\n",
    "            \"axes.titlesize\":16,\n",
    "            \"axes.labelsize\":16,\n",
    "            \"xtick.labelsize\": 16.0,\n",
    "            \"ytick.labelsize\": 16.0,\n",
    "            \"legend.fontsize\": 16.0})\n",
    "palette_ = sns.color_palette(\"Set1\")\n",
    "palette = palette_[2:5] + palette_[7:]\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Utilities\n",
    "\n",
    "from general_utils import (\n",
    "  ModelAndTokenizer,\n",
    "  make_inputs,\n",
    "  decode_tokens,\n",
    "  find_token_range,\n",
    "  predict_from_input,\n",
    ")\n",
    "\n",
    "from patchscopes_utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LQX9Qx6GGdLZ"
   },
   "outputs": [],
   "source": [
    "model_to_hook = {\n",
    "    \"EleutherAI/pythia-12b\": set_hs_patch_hooks_neox,\n",
    "    \"meta-llama/Llama-2-13b-hf\": set_hs_patch_hooks_llama,\n",
    "    \"lmsys/vicuna-7b-v1.5\": set_hs_patch_hooks_llama,\n",
    "    \"./stable-vicuna-13b\": set_hs_patch_hooks_llama,\n",
    "    \"CarperAI/stable-vicuna-13b-delta\": set_hs_patch_hooks_llama,\n",
    "    \"EleutherAI/gpt-j-6b\": set_hs_patch_hooks_gptj\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Hugging Face cache directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/students/kolber/seminars/kolber/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4479f16b9a544b79bb8790693701d8de",
      "708c70001ba64b3196bf2ced4fe01f6c"
     ]
    },
    "id": "fKGGJO3GQ3in",
    "outputId": "aed82adb-d542-4de6-ade7-c2a4f7aadcc6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b90d35d2ff44cbf9643eb8f19988b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695942fc16c84864b7a140607427f314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211472629e5846ed90db279f16ff6754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd766986a0694befa75de842c9a21dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55124b8bfdfe430cb8477e4767b8a423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/49.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a116029376704ee5aaca86ef89d5ee4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebb6fb57dfa48c7bc1fa0b9a1c839de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/9.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616bdf375c324ebaaad93960243ec755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/9.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a54437069047139707159395d6f9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ec6c3eb487477bb6782520a37c78b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m mt \u001b[38;5;241m=\u001b[39m \u001b[43mModelAndTokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m mt\u001b[38;5;241m.\u001b[39mset_hs_patch_hooks \u001b[38;5;241m=\u001b[39m model_to_hook[model_name]\n\u001b[1;32m     17\u001b[0m mt\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/mnt/proj/kolber/interpretability/patchscopes/code/general_utils.py:61\u001b[0m, in \u001b[0;36mModelAndTokenizer.__init__\u001b[0;34m(self, model_name, model, tokenizer, low_cpu_mem_usage, torch_dtype, use_fast, device, cache_dir)\u001b[0m\n\u001b[1;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     55\u001b[0m     model_name,\n\u001b[1;32m     56\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m     57\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m     58\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m set_requires_grad(\u001b[38;5;28;01mFalse\u001b[39;00m, model)\n\u001b[1;32m     63\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/GLM/lib/python3.9/site-packages/transformers/modeling_utils.py:1811\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1807\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1809\u001b[0m     )\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GLM/lib/python3.9/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GLM/lib/python3.9/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GLM/lib/python3.9/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/GLM/lib/python3.9/site-packages/torch/cuda/__init__.py:314\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    313\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "model_name = \"EleutherAI/pythia-12b\"\n",
    "sos_tok = False\n",
    "\n",
    "if \"13b\" in model_name or \"12b\" in model_name:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None\n",
    "\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch_dtype,\n",
    ")\n",
    "mt.set_hs_patch_hooks = model_to_hook[model_name]\n",
    "mt.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsZLB6l_GdLa"
   },
   "source": [
    "# Next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "YdeaeZVjGdLb",
    "outputId": "4923c9ff-f31c-410f-f58f-955285533ab9"
   },
   "outputs": [],
   "source": [
    "# load and filter the pile dataset\n",
    "\n",
    "pile_dataset = datasets.load_from_disk('./the_pile_deduplicated')\n",
    "print(len(pile_dataset))\n",
    "pile_dataset = pile_dataset.filter(\n",
    "    lambda x: len(x['text'].split(' ')) < 250 and len(x['text']) < 2000 # keep only texts with less than 250 tokens and less than 2000 characters\n",
    ").shuffle(seed=42)\n",
    "print(len(pile_dataset))\n",
    "\n",
    "trn_n = 10000\n",
    "val_n = 2000\n",
    "pile_trn = pile_dataset['text'][:trn_n]\n",
    "pile_val = pile_dataset['text'][trn_n:trn_n+val_n]\n",
    "sentences = [(x, 'train') for x in pile_trn] + [(x, 'validation') for x in pile_val] # result is a list of tuples (text, 'train'/'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtLHs-6OGdLb"
   },
   "outputs": [],
   "source": [
    "# get hidden states for all layers of the model for each sentence in the dataset and save them to a pickle file.\n",
    "# only source part of the patchscope operation\n",
    "\n",
    "data = {}\n",
    "for sentence, split in tqdm(sentences):\n",
    "    inp = make_inputs(mt.tokenizer, [sentence], device=mt.model.device)\n",
    "    if sos_tok:\n",
    "        start_pos = 1\n",
    "    else:\n",
    "        start_pos = 0\n",
    "    position = random.randint(start_pos, len(inp['input_ids'][0]) - 1)\n",
    "\n",
    "    if (sentence, position, split) not in data:\n",
    "        output = mt.model(**inp, output_hidden_states = True)\n",
    "\n",
    "        data[(sentence, position, split)] =  [\n",
    "            output[\"hidden_states\"][layer+1][0][position].detach().cpu().numpy()\n",
    "            for layer in range(mt.num_layers)\n",
    "        ]\n",
    "\n",
    "df = pd.Series(data).reset_index()\n",
    "df.columns = ['full_text', 'position', 'data_split', 'hidden_rep']\n",
    "\n",
    "df.to_pickle(model_name+\"_pile_trn_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGjOSASJGdLc",
    "outputId": "d3d5c0c6-57a1-4aac-f4c3-b98a9d26bc02"
   },
   "outputs": [],
   "source": [
    "# get hidden states for all layers of the model for each sentence in the dataset and save them to a pickle file.\n",
    "# source and target parts of the patchscope operation\n",
    "\n",
    "prompt_target = \"cat -> cat\\n1135 -> 1135\\nhello -> hello\\n?\"\n",
    "inp_target = make_inputs(mt.tokenizer, [prompt_target], device=mt.model.device)\n",
    "\n",
    "data = {}\n",
    "for sentence, split in tqdm(sentences):\n",
    "    inp = make_inputs(mt.tokenizer, [sentence], device=mt.model.device)\n",
    "    if sos_tok:\n",
    "        start_pos = 1\n",
    "    else:\n",
    "        start_pos = 0\n",
    "    position = random.randint(start_pos, len(inp['input_ids'][0]) - 2)\n",
    "\n",
    "    if (sentence, position, split, \"source\") not in data:\n",
    "        output = mt.model(**inp, output_hidden_states = True)\n",
    "        _, answer_t = torch.max(torch.softmax(output.logits[0, -1, :], dim=0), dim=0)\n",
    "        data[(sentence, position, split, \"source\")] =  [\n",
    "            output[\"hidden_states\"][layer+1][0][position].detach().cpu().numpy()\n",
    "            for layer in range(mt.num_layers)\n",
    "        ]\n",
    "\n",
    "        inp_target['input_ids'][0][-1] = answer_t\n",
    "        output = mt.model(**inp_target, output_hidden_states = True)\n",
    "        data[(sentence, position, split, \"target\")] =  [\n",
    "            output[\"hidden_states\"][layer+1][0][-1].detach().cpu().numpy()\n",
    "            for layer in range(mt.num_layers)\n",
    "        ]\n",
    "\n",
    "df = pd.Series(data).reset_index()\n",
    "df.columns = ['full_text', 'position', 'data_split', 'prompt', 'hidden_rep']\n",
    "\n",
    "df.to_pickle(model_name+\"_pile_trn_val.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEemdkGOGdLd"
   },
   "outputs": [],
   "source": [
    "# Pad and unpad \n",
    "\n",
    "pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "unpad = lambda x: x[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_TIyUtoGdLd",
    "outputId": "a9d67b1c-3ba9-4e8b-e3d1-e45b8fdbd6c9"
   },
   "outputs": [],
   "source": [
    "# Across layer mappings\n",
    "\n",
    "output_dir = f'{model_name}_mappings_pile'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_trn = pd.DataFrame(df[df['data_split'] == 'train']['hidden_rep'].to_list(),\n",
    "                      columns=[layer for layer in range(mt.num_layers)])\n",
    "\n",
    "target_layer = mt.num_layers - 1\n",
    "Y = np.array(\n",
    "    df_trn[target_layer].values.tolist()\n",
    ")\n",
    "\n",
    "mappings = []\n",
    "for layer in range(mt.num_layers):\n",
    "    X = np.array(\n",
    "        df_trn[layer].values.tolist()\n",
    "    )\n",
    "\n",
    "    # Solve the least squares problem X * A = Y\n",
    "    # to find our transformation matrix A\n",
    "    A, res, rank, s = np.linalg.lstsq(pad(X), pad(Y))\n",
    "    transform = lambda x: unpad(pad(x) @ A)\n",
    "\n",
    "    mappings.append(A)\n",
    "    with open(f'{output_dir}/mapping_{layer}-{target_layer}.npy', 'wb') as fd:\n",
    "        np.save(fd, A)\n",
    "\n",
    "    print(layer, \"max error on train:\", np.abs(Y - transform(X)).max())\n",
    "\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoEZwZLEGdLd",
    "outputId": "afbbe9a9-596a-4e8d-8689-96b1a2758a7a"
   },
   "outputs": [],
   "source": [
    "# Prompt-id mappings\n",
    "\n",
    "output_dir = f'{model_name}_mappings_pile_prompt-id'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_trn_src = pd.DataFrame(df[(df['data_split'] == 'train') & (df['prompt'] == 'source')]['hidden_rep'].to_list(),\n",
    "                          columns=[layer for layer in range(mt.num_layers)])\n",
    "df_trn_tgt = pd.DataFrame(df[(df['data_split'] == 'train') & (df['prompt'] == 'target')]['hidden_rep'].to_list(),\n",
    "                          columns=[layer for layer in range(mt.num_layers)])\n",
    "\n",
    "mappings = []\n",
    "for layer in range(mt.num_layers):\n",
    "    X = np.array(\n",
    "        df_trn_src[layer].values.tolist()\n",
    "    )\n",
    "    Y = np.array(\n",
    "        df_trn_tgt[layer].values.tolist()\n",
    "    )\n",
    "\n",
    "    # Solve the least squares problem X * A = Y\n",
    "    # to find our transformation matrix A\n",
    "    A, res, rank, s = np.linalg.lstsq(pad(X), pad(Y))\n",
    "    transform = lambda x: unpad(pad(x) @ A)\n",
    "\n",
    "    mappings.append(A)\n",
    "    with open(f'{output_dir}/mapping_{layer}.npy', 'wb') as fd:\n",
    "        np.save(fd, A)\n",
    "\n",
    "    print(layer, \"max error on train:\", np.abs(Y - transform(X)).max())\n",
    "\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UhNsl2HGdLd"
   },
   "outputs": [],
   "source": [
    "mappings = []\n",
    "for layer in tqdm(range(mt.num_layers)):\n",
    "    with open(f'{model_name}_mappings_pile/mapping_{layer}-{mt.num_layers-1}.npy', 'rb') as fd:\n",
    "        A = np.load(fd)\n",
    "    mappings.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH8FA6WsGdLe"
   },
   "outputs": [],
   "source": [
    "# Evaluate linear mappings on the validation set of WikiText\n",
    "device = mt.model.device\n",
    "target_layer = mt.num_layers - 1\n",
    "\n",
    "records = []\n",
    "for layer in tqdm(range(mt.num_layers)):\n",
    "    A = mappings[layer]\n",
    "    transform = lambda x: torch.tensor(\n",
    "        np.squeeze(\n",
    "            unpad(np.dot(\n",
    "                pad(np.expand_dims(x.detach().cpu().numpy(), 0)),\n",
    "                A\n",
    "            ))\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "    for idx, row in df[df['data_split'] == 'validation'].iterrows():\n",
    "        prompt = row['full_text']\n",
    "        position = row['position']\n",
    "        prec_1, surprisal = evaluate_patch_next_token_prediction(\n",
    "            mt, prompt, prompt, layer, target_layer,\n",
    "            position, position, position_prediction=position, transform=transform)\n",
    "\n",
    "        records.append({'layer': layer, 'prec_1': prec_1, 'surprisal': surprisal})\n",
    "\n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name}_mappings_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNVyKXLAGdLe",
    "outputId": "9d8a9374-5bd5-4ad8-cadf-18831f3c8846"
   },
   "outputs": [],
   "source": [
    "# Evaluate identity mapping on the validation set of WikiText\n",
    "\n",
    "target_layer = mt.num_layers - 1\n",
    "\n",
    "records = []\n",
    "for layer in tqdm(range(mt.num_layers)):\n",
    "    for idx, row in df[df['data_split'] == 'validation'].iterrows():\n",
    "        prompt = row['full_text']\n",
    "        position = row['position']\n",
    "        prec_1, surprisal = evaluate_patch_next_token_prediction(\n",
    "            mt, prompt, prompt, layer, target_layer,\n",
    "            position, position, position_prediction=position)\n",
    "\n",
    "        records.append({'layer': layer, 'prec_1': prec_1, 'surprisal': surprisal})\n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name}_identity_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuKvnceYGdLe"
   },
   "outputs": [],
   "source": [
    "# Evaluate the ID prompt on the validation set of WikiText (with/without mappings)\n",
    "\n",
    "device = mt.model.device\n",
    "\n",
    "prompt_target = \"cat -> cat\\n1135 -> 1135\\nhello -> hello\\n?\"\n",
    "position_target = -1\n",
    "apply_mappings = True\n",
    "\n",
    "records = []\n",
    "for layer in tqdm(range(mt.num_layers)):\n",
    "    if apply_mappings:\n",
    "        A = mappings[layer]\n",
    "        transform = lambda x: torch.tensor(\n",
    "            np.squeeze(\n",
    "                unpad(np.dot(\n",
    "                    pad(np.expand_dims(x.detach().cpu().numpy(), 0)),\n",
    "                    A\n",
    "                ))\n",
    "            )\n",
    "        ).to(device)\n",
    "    else:\n",
    "        transform = None\n",
    "\n",
    "    for idx, row in df[df['data_split'] == 'validation'].iterrows():\n",
    "        if 'prompt' in row and row['prompt'] == 'target':\n",
    "            continue\n",
    "        prompt_source = row['full_text']\n",
    "        position_source = row['position']\n",
    "        prec_1, surprisal = evaluate_patch_next_token_prediction(\n",
    "            mt, prompt_source, prompt_target, layer, layer,\n",
    "            position_source, position_target, position_prediction=position_target, transform=transform)\n",
    "\n",
    "        records.append({'layer': layer, 'prec_1': prec_1, 'surprisal': surprisal})\n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "if apply_mappings:\n",
    "    results.to_csv(f'{model_name}_prompt-id-mapping_pile_eval.csv')\n",
    "else:\n",
    "    results.to_csv(f'{model_name}_prompt-id_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4v3LlWQGdLe",
    "outputId": "e4bb6dee-10ba-4792-bf80-54299bcf66a5"
   },
   "outputs": [],
   "source": [
    "results1 = pd.read_csv(f'{model_name}_identity_pile_eval.csv')\n",
    "results1[\"variant\"] = \"identity\"\n",
    "results2 = pd.read_csv(f'{model_name}_mappings_pile_eval.csv')\n",
    "results2[\"variant\"] = \"affine mapping\"\n",
    "results3 = pd.read_csv(f'{model_name}_prompt-id_pile_eval.csv')\n",
    "results3[\"variant\"] = \"prompt id\"\n",
    "\n",
    "results = pd.concat([results1, results2, results3], ignore_index=True)\n",
    "\n",
    "for metric in ['prec_1', 'surprisal']:\n",
    "    ax = sns.lineplot(data=results, x='layer', y=metric, hue=\"variant\")\n",
    "    ax.set_title(model_name.strip('./'))\n",
    "    ax.legend_.set_title('')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
